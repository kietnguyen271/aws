<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="thienlh@thienlu.com"><link rel=icon href=../../images/favicon.png type=image/png><title>Blog 3 :: Internship Report</title>
<link href=../../css/nucleus.css?1765252493 rel=stylesheet><link href=../../css/fontawesome-all.min.css?1765252493 rel=stylesheet><link href=../../css/hybrid.css?1765252493 rel=stylesheet><link href=../../css/featherlight.min.css?1765252493 rel=stylesheet><link href=../../css/perfect-scrollbar.min.css?1765252493 rel=stylesheet><link href=../../css/auto-complete.css?1765252493 rel=stylesheet><link href=../../css/atom-one-dark-reasonable.css?1765252493 rel=stylesheet><link href=../../css/theme.css?1765252493 rel=stylesheet><link href=../../css/hugo-theme.css?1765252493 rel=stylesheet><link href=../../css/theme-workshop.css?1765252493 rel=stylesheet><script src=../../js/jquery-3.3.1.min.js?1765252493></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=../../3-blogstranslated/3.3-blog3/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=../../><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=../../js/lunr.min.js?1765252493></script><script type=text/javascript src=../../js/auto-complete.js?1765252493></script><script type=text/javascript>var baseurl="https://thienluhoan.github.io/workshop-template/"</script><script type=text/javascript src=../../js/search.js?1765252493></script></div><div class=highlightable><ul class=topics><li data-nav-id=/1-worklog/ title="Worklog Overview" class=dd-item><a href=../../1-worklog/><b>1. </b>Worklog Overview
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/1-worklog/1.1-week1/ title="Week 1 Worklog" class=dd-item><a href=../../1-worklog/1.1-week1/><b>1.1. </b>Week 1 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.2-week2/ title="Week 2 Worklog" class=dd-item><a href=../../1-worklog/1.2-week2/><b>1.2. </b>Week 2 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.3-week3/ title="Week 3 Worklog" class=dd-item><a href=../../1-worklog/1.3-week3/><b>1.3. </b>Week 3 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.4-week4/ title="Week 4 Worklog" class=dd-item><a href=../../1-worklog/1.4-week4/><b>1.4. </b>Week 4 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.5-week5/ title="Week 5 Worklog" class=dd-item><a href=../../1-worklog/1.5-week5/><b>1.5. </b>Week 5 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.6-week6/ title="Week 6 Worklog" class=dd-item><a href=../../1-worklog/1.6-week6/><b>1.6. </b>Week 6 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.7-week7/ title="Week 7 Worklog" class=dd-item><a href=../../1-worklog/1.7-week7/><b>1.7. </b>Week 7 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.8-week8/ title="Week 8 Worklog" class=dd-item><a href=../../1-worklog/1.8-week8/><b>1.8. </b>Week 8 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.9-week9/ title="Week 9 Worklog" class=dd-item><a href=../../1-worklog/1.9-week9/><b>1.9. </b>Week 9 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.10-week10/ title="Week 10 Worklog" class=dd-item><a href=../../1-worklog/1.10-week10/><b>1.10. </b>Week 10 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.11-week11/ title="Week 11 Worklog" class=dd-item><a href=../../1-worklog/1.11-week11/><b>1.11. </b>Week 11 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week12/ title="Week 12 Worklog" class=dd-item><a href=../../1-worklog/1.12-week12/><b>1.12. </b>Week 12 Worklog
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/2-proposal/ title=Proposal class=dd-item><a href=../../2-proposal/><b>2. </b>Proposal
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/ title=Blogs class="dd-item
parent"><a href=../../3-blogstranslated/><b>3. </b>Blogs
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/3-blogstranslated/3.1-blog1/ title="Blog 1" class=dd-item><a href=../../3-blogstranslated/3.1-blog1/><b>3.1. </b>Blog 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.2-blog2/ title="Blog 2" class=dd-item><a href=../../3-blogstranslated/3.2-blog2/><b>3.2. </b>Blog 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.3-blog3/ title="Blog 3" class="dd-item
active"><a href=../../3-blogstranslated/3.3-blog3/><b>3.3. </b>Blog 3
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/4-eventparticipated/ title="Events Participated" class=dd-item><a href=../../4-eventparticipated/><b>4. </b>Events Participated
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/4-eventparticipated/4.1-event1/ title="AWS Cloud Day Vietnam - AI Edition 2025" class=dd-item><a href=../../4-eventparticipated/4.1-event1/><b>4.1. </b>AWS Cloud Day Vietnam - AI Edition 2025
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.2-event2/ title="Discover Agentic AI – Amazon QuickSuite Workshop" class=dd-item><a href=../../4-eventparticipated/4.2-event2/><b>4.2. </b>Discover Agentic AI – Amazon QuickSuite Workshop
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.3-event3/ title="AWS Cloud Mastery Series #3 - Security Pillar Deep Dive" class=dd-item><a href=../../4-eventparticipated/4.3-event3/><b>4.3. </b>AWS Cloud Mastery Series #3 - Security Pillar Deep Dive
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.4-event4/ title="AWS Cloud Mastery Series #2 - DevOps on AWS" class=dd-item><a href=../../4-eventparticipated/4.4-event4/><b>4.4. </b>AWS Cloud Mastery Series #2 - DevOps on AWS
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.5-event5/ title="AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS" class=dd-item><a href=../../4-eventparticipated/4.5-event5/><b>4.5. </b>AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.6-event6/ title="AWS Cloud Mastery Series #1" class=dd-item><a href=../../4-eventparticipated/4.6-event6/><b>4.6. </b>AWS Cloud Mastery Series #1
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/ title=Workshop class=dd-item><a href=../../5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.1-introduction/ title="5.1 Introduction" class=dd-item><a href=../../5-workshop/5.1-introduction/><b>5.1. </b>5.1 Introduction
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.2-perequisites/ title="5.2 Prerequisites" class=dd-item><a href=../../5-workshop/5.2-perequisites/><b>5.2. </b>5.2 Prerequisites
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.3-setup-cognito/ title="5.3 Setup Cognito" class=dd-item><a href=../../5-workshop/5.3-setup-cognito/><b>5.3. </b>5.3 Setup Cognito
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-setup-lambda/ title="5.4 Setup Lambda" class=dd-item><a href=../../5-workshop/5.4-setup-lambda/><b>5.4. </b>5.4 Setup Lambda
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.5-setup-api-gateway/ title="5.5 Setup API Gateway" class=dd-item><a href=../../5-workshop/5.5-setup-api-gateway/><b>5.5. </b>5.5 Setup API Gateway
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.6-setup-rds-database/ title="5.6 Setup RDS Database" class=dd-item><a href=../../5-workshop/5.6-setup-rds-database/><b>5.6. </b>5.6 Setup RDS Database
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.7-setup-s3-cloudfront/ title="5.7 Setup S3 + CloudFront" class=dd-item><a href=../../5-workshop/5.7-setup-s3-cloudfront/><b>5.7. </b>5.7 Setup S3 + CloudFront
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.8-setup-vpc-security/ title="Setup VPC & Security" class=dd-item><a href=../../5-workshop/5.8-setup-vpc-security/><b>5.8. </b>Setup VPC & Security
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.9-monitoring-logging/ title="Monitoring & Logging" class=dd-item><a href=../../5-workshop/5.9-monitoring-logging/><b>5.9. </b>Monitoring & Logging
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.10-cleanup/ title="Cleanup & Cost Optimization" class=dd-item><a href=../../5-workshop/5.10-cleanup/><b>5.10. </b>Cleanup & Cost Optimization
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/aws_resources_inventory/ title class=dd-item><a href=../../5-workshop/aws_resources_inventory/><i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/6-self-evaluation/ title=Self-Assessment class=dd-item><a href=../../6-self-evaluation/><b>6. </b>Self-Assessment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/7-feedback/ title="Feedback & Suggestions" class=dd-item><a href=../../7-feedback/><b>7. </b>Feedback & Suggestions
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/ selected>English</option><option id=vi value=https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.3-blog3/>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate=today.toLocaleDateString("en-GB");document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=../../>Internship Report</a> > <a href=../../3-blogstranslated/>Blogs</a> > Blog 3</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#solution-overview>Solution overview</a></li><li><a href=#prerequisites>Prerequisites</a></li></ul><ul><li><a href=#configuring-iam-policy>Configuring IAM policy</a></li><li><a href=#required-dependent-libraries>Required dependent libraries</a></li><li><a href=#configuring-spark-streaming-job-for-amazon-msk-iam-authentication>Configuring Spark Streaming job for Amazon MSK IAM authentication</a></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Blog 3</h1><h1 id=building-a-secure-serverless-streaming-pipeline-with-amazon-msk-serverless-amazon-emr-serverless-and-iam>Building a secure serverless streaming pipeline with Amazon MSK Serverless, Amazon EMR Serverless, and IAM</h1><p><em>by Shubham Purwar, Nitin Kumar, and Prashanthi Chinthala on 02 JUN 2025 in<br><a href=https://aws.amazon.com/athena/>Amazon Athena</a>, <a href=https://aws.amazon.com/emr/>Amazon EMR</a>,<br><a href=https://aws.amazon.com/msk/>Amazon Managed Streaming for Apache Kafka (Amazon MSK)</a>,<br>Analytics, AWS Big Data<br>Permalink • Comments • Share</em></p><p>The exponential growth and massive volume of streaming data have made it an essential resource for organizations around the world.<br>To fully harness this potential, real-time analytics is critical to extract actionable insights.</p><p>Generated from various sources including social networks, Internet of Things (IoT) sensors, and user interactions, streaming data enables businesses to quickly respond to emerging trends and events, make informed decisions, and maintain competitive advantage.</p><p>Typically, streaming applications use Apache Kafka for data ingestion and <a href=https://spark.apache.org/structured-streaming/>Apache Spark Structured Streaming</a> for processing.<br>However, integrating and securing these components introduces significant challenges for users.</p><p>The complexity of managing certificates, keystores, and TLS configurations required to connect Spark Streaming with Kafka brokers often demands deep technical expertise.</p><p>A managed serverless framework can greatly simplify this process, eliminating the need for manual configuration and enabling seamless integration between critical components.</p><p>To simplify management and security in traditional streaming architectures, you can use <a href=https://aws.amazon.com/msk/>Amazon Managed Streaming for Apache Kafka</a> (Amazon MSK).</p><p>This fully managed service streamlines the ingestion and processing of data.<br><a href=https://aws.amazon.com/msk/serverless/>Amazon MSK Serverless</a> removes the need for cluster management and automatic scaling, while also strengthening security by integrating with <a href=https://aws.amazon.com/iam/>AWS Identity and Access Management</a> (IAM) for authentication and authorization.</p><p>This unified approach replaces the complex process of managing certificates and security keys required by TLS client authentication through <a href=https://aws.amazon.com/certificate-manager/>AWS Certificate Manager</a>, simplifying operations and enhancing data protection.</p><p>For example, when a client attempts to send data into a cluster, MSK Serverless verifies the identity and access permissions of the client through IAM.</p><p>To efficiently process data, you can use <a href=https://aws.amazon.com/emr/serverless/>Amazon EMR Serverless</a> along with a Spark application built using Spark Structured Streaming, enabling near real-time processing.<br>This configuration seamlessly handles large volumes of data from MSK Serverless, using IAM authentication to ensure high performance and strong security during processing.</p><p>This blog presents a comprehensive end-to-end solution for processing data from MSK Serverless using EMR Serverless Spark Streaming jobs secured with IAM authentication.<br>Additionally, it demonstrates how to query processed data using <a href=https://aws.amazon.com/athena/>Amazon Athena</a>, providing an integrated and streamlined data processing and analytics workflow.</p><p>This solution enables near real-time querying of the latest processed data from MSK Serverless and EMR Serverless through Athena, delivering instant insights and analytics.</p><h2 id=solution-overview>Solution overview</h2><p>The diagram below illustrates the architecture you will build in this blog.<br><img alt=architecture-diagram src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/blog-4406-image-1.png></p><p>The workflow consists of the following steps:</p><ol><li><p>The architecture begins with an MSK Serverless cluster configured with IAM authentication.<br>An <a href=https://aws.amazon.com/ec2/>Amazon Elastic Compute Cloud</a> (Amazon EC2) instance running a Python script <code>producer.py</code> acts as the data producer, sending sample data to a Kafka topic in the cluster.</p></li><li><p>A Spark Streaming job reads data from the Kafka topic, stores it in <a href=https://aws.amazon.com/s3/>Amazon Simple Storage Service</a> (Amazon S3), and creates corresponding tables in the <a href=https://aws.amazon.com/glue/>AWS Glue Data Catalog</a>.<br>While continuously consuming data from the Kafka topic, the job keeps updating new incoming data.<br>With checkpointing enabled, the job tracks processed records and can resume from the last checkpoint in case of failures, ensuring seamless data processing.</p></li><li><p>To analyze the data, users can leverage Athena — a serverless query service.<br>Athena allows interactive SQL queries to be run directly on data stored in Amazon S3, without requiring complex infrastructure.</p></li></ol><h2 id=prerequisites>Prerequisites</h2><p>Before starting, ensure that you have:</p><ul><li>An active AWS account with billing enabled</li><li>An IAM user with administrator access (AdministratorAccess policy) or specific permissions to create and manage resources such as VPC, subnet, security group, IAM roles, NAT gateway, internet gateway, EC2 client, MSK Serverless, EMR Serverless, <a href=https://aws.amazon.com/emr/features/emr-studio/>Amazon EMR Studio</a>, and S3 buckets</li><li>Sufficient VPC capacity in your selected AWS Region</li></ul><p>Although using an IAM user with administrator access works fine,<br>it is recommended that in production environments you follow the principle of least privilege — by creating custom IAM policies containing only required permissions.<br>The IAM user created for this tutorial is granted AdministratorAccess, but such elevated permissions are not strictly necessary.</p><p>For this blog, we will create the solution resources in the <strong>us-east-2</strong> Region using <a href=https://aws.amazon.com/cloudformation/>AWS CloudFormation</a> templates.<br>In the following sections, we guide you through configuring resources and deploying the solution.</p><hr><h1 id=create-msk-serverless-and-emr-serverless-resources>Create MSK Serverless and EMR Serverless resources</h1><p>The <strong>vpc-msk-emr-serverless-studio.yaml</strong> stack will create the following resources:<br>VPC, subnets, security groups, IAM roles, NAT gateway, internet gateway, EC2 client, MSK Serverless, EMR Serverless, EMR Studio, and S3 buckets.</p><p>To create the resources for this solution, follow these steps:</p><ol><li><p><strong>Launch the vpc-msk-emr-serverless-studio stack</strong> using the CloudFormation template.<br><a href=https://signin.aws.amazon.com/>Amazon Web Services Sign-In</a></p></li><li><p><strong>Provide parameter values</strong> as listed in the table below:</p></li></ol><table><thead><tr><th style=text-align:left>Parameters</th><th style=text-align:left>Description</th><th style=text-align:left>Sample value</th></tr></thead><tbody><tr><td style=text-align:left><strong>EnvironmentName</strong></td><td style=text-align:left>Environment name prefix for resources.</td><td style=text-align:left>msk-emr-serverless-pipeline</td></tr><tr><td style=text-align:left><strong>InstanceType</strong></td><td style=text-align:left>EC2 instance type for the <strong>Amazon MSK client</strong>.</td><td style=text-align:left>t2.micro</td></tr><tr><td style=text-align:left><strong>LatestAmiId</strong></td><td style=text-align:left>Latest <strong>Amazon Linux 2023 AMI</strong> ID for EC2. You may use default value.</td><td style=text-align:left>/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64</td></tr><tr><td style=text-align:left><strong>VpcCIDR</strong></td><td style=text-align:left>IP address range (<strong>CIDR notation</strong>) for the VPC.</td><td style=text-align:left>10.192.0.0/16</td></tr></tbody></table><table><thead><tr><th style=text-align:left>Parameters</th><th style=text-align:left>Description</th><th style=text-align:left>Sample value</th></tr></thead><tbody><tr><td style=text-align:left><strong>PublicSubnet1CIDR</strong></td><td style=text-align:left>IP range for the <strong>public subnet</strong> in the <strong>first Availability Zone</strong>.</td><td style=text-align:left>10.192.10.0/24</td></tr><tr><td style=text-align:left><strong>PublicSubnet2CIDR</strong></td><td style=text-align:left>IP range for the <strong>public subnet</strong> in the <strong>second Availability Zone</strong>.</td><td style=text-align:left>10.192.11.0/24</td></tr><tr><td style=text-align:left><strong>PrivateSubnet1CIDR</strong></td><td style=text-align:left>IP range for the <strong>private subnet</strong> in the <strong>first Availability Zone</strong>.</td><td style=text-align:left>10.192.20.0/24</td></tr></tbody></table><table><thead><tr><th style=text-align:left>Parameters</th><th style=text-align:left>Description</th><th style=text-align:left>Sample value</th></tr></thead><tbody><tr><td style=text-align:left><strong>PrivateSubnet2CIDR</strong></td><td style=text-align:left>IP range for the private subnet in the second Availability Zone.</td><td style=text-align:left>10.192.21.0/24</td></tr></tbody></table><p>The stack creation process may take about 10 minutes.</p><p>After the stack is created, check the <strong>Outputs</strong> tab to view the stack output values.</p><p><img alt=Outputs src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/blog-4406-image-4.jpg></p><p>Next, you will configure the data ingestion process to send data into a Kafka topic from the Kafka EC2 instance.</p><hr><h1 id=produce-records-to-kafka-topic>Produce records to Kafka topic</h1><p>Follow these steps to configure data ingestion:</p><ol><li><p>Access the Amazon EC2 console and navigate to the EC2 instance created via the CloudFormation template.</p><p><img alt="EC2 console" src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/blog-4406-image-5.jpeg></p></li><li><p>Log in to the EC2 instance using Session Manager, a feature of AWS Systems Manager.</p></li><li><p>Select the instance named <strong>msk-emr-serverless-blog</strong>, then choose <strong>Connect</strong>.</p><p><img alt="Session Manager" src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/blog-4406-image-6.jpg></p></li><li><p>Create a <strong>Kafka topic</strong> in <strong>MSK Serverless</strong> from the EC2 instance.</p></li></ol><p><strong>a.</strong> In the export command below, replace <strong>my-endpoint</strong> with the value of <strong>MSKBootstrapServers</strong> displayed in the CloudFormation Outputs:</p><pre tabindex=0><code>$ sudo su - ec2-user
$ BS=&lt;your-msk-serverless-endpoint (e.g.) boot-xxxxxx.yy.kafka-serverless.us-east-2.amazonaws.com:9098&gt;
</code></pre><p><strong>b.</strong> Run the following command on the EC2 instance to create a Kafka topic named <code>sales_data_topic</code>.</p><p>Kafka client is already installed in the directory <code>/home/ec2-user</code> for the ec2-user, along with the MSK IAM Authentication JAR and the client configuration file located at:</p><pre tabindex=0><code>/home/ec2-user/kafka_2.12-2.8.1/bin/client.properties
</code></pre><p>The following shows the content of <code>client.properties</code>:</p><pre tabindex=0><code>security.protocol=SASL_SSL
sasl.mechanism=AWS_MSK_IAM
sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;
sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler
</code></pre><p>Create topic command:</p><pre tabindex=0><code>/home/ec2-user/kafka_2.12-2.8.1/bin/kafka-topics.sh \
--bootstrap-server $BS \
--command-config /home/ec2-user/kafka_2.12-2.8.1/bin/client.properties \
--create --topic sales_data_topic \
--partitions 10
</code></pre><p>Output:</p><pre tabindex=0><code>Created topic sales_data_topic.
</code></pre><ol start=5><li>Run the following command to produce records into the Kafka topic using the Python script <code>syntheticSalesDataProducer.py</code> available in the EC2 instance.<br>Update the <strong>Region</strong> value based on the AWS Region you are using.</li></ol><pre tabindex=0><code>nohup python3 -u syntheticSalesDataProducer.py --num_records 1000 \
--sales_data_topic sales_data_topic --bootstrap_server $BS \
--region=us-east-2 &gt; syntheticSalesDataProducer.log &amp;
</code></pre><hr><h1 id=understanding-amazon-msk-iam-authentication-with-emr-serverless>Understanding Amazon MSK IAM authentication with EMR Serverless</h1><p>Amazon MSK IAM authentication enables secure authentication and authorization for Kafka clusters (MSK Serverless) using IAM roles.</p><p>When integrated with EMR Serverless Spark Streaming, Amazon MSK IAM authentication allows Spark jobs to safely access Kafka topics, using IAM roles for fine-grained access control.<br>This ensures highly secure data processing and streaming.</p><hr><h2 id=configuring-iam-policy>Configuring IAM policy</h2><p>To allow EMR Serverless jobs to authenticate with the MSK Serverless cluster via IAM, you must attach Kafka-related permissions to the EMR Serverless job execution role.</p><p>These permissions enable the job to perform necessary operations on the Kafka cluster, Kafka topics, and consumer groups.</p><p>The following IAM policy must be attached to the EMR Serverless job execution role:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;Version&#34;</span>: <span style=color:#e6db74>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;Statement&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Action&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:Connect&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:DescribeCluster&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Resource&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;arn:aws:kafka:&lt;AWS-REGION&gt;:&lt;ACCOUNTID&gt;:cluster/&lt;SERVERLESS_CLUSTER_NAME&gt;/&lt;ID&gt;&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Action&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:CreateTopic&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:DescribeTopic&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:WriteData&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:ReadData&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Resource&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;arn:aws:kafka:&lt;AWS-REGION&gt;:&lt;ACCOUNTID&gt;:topic/&lt;SERVERLESS_CLUSTER_NAME&gt;/*/*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Action&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:AlterGroup&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;kafka-cluster:DescribeGroup&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Resource&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;arn:aws:kafka:&lt;AWS-REGION&gt;:&lt;ACCOUNTID&gt;:group/&lt;SERVERLESS_CLUSTER_NAME&gt;/*/*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The configuration above relates to the following <strong>actions</strong>:</p><ul><li><strong>Connect, DescribeCluster</strong> – Required to <strong>initiate secure connections</strong> and retrieve metadata.</li><li><strong>DescribeTopic, ReadData, WriteData</strong> – Allow <strong>reading and writing data</strong> (consuming and producing data).</li><li><strong>CreateTopic (optional)</strong> – Allows <strong>dynamic topic creation</strong> if necessary.</li><li><strong>AlterGroup, DescribeGroup</strong> – Required for <strong>managing consumer groups</strong> in <strong>streaming jobs</strong>.</li></ul><p>These permissions ensure that the Spark Streaming job can <strong>authenticate and interact securely</strong> with MSK Serverless resources using its IAM role.</p><hr><h2 id=required-dependent-libraries>Required dependent libraries</h2><p>To enable Amazon MSK IAM authentication in Spark (especially on EMR Serverless), include the following JAR dependencies in the Spark Streaming job via the <strong>sparkSubmitParameters</strong>:</p><ul><li><a href=https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12>spark-sql-kafka-0-10_2.12</a> – Kafka connector for Spark Structured Streaming, providing DataFrame APIs to read and write Kafka data.</li><li><a href=https://mvnrepository.com/artifact/software.amazon.msk/aws-msk-iam-auth>aws-msk-iam-auth</a> – Provides IAM authentication mechanism required to connect to MSK Serverless using <strong>AWS_MSK_IAM SASL mechanism</strong>.</li></ul><p>You can include these dependencies directly using the <strong>&ndash;packages</strong> option when submitting the EMR Serverless job.<br>Example:</p><pre tabindex=0><code>packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,software.amazon.msk:aws-msk-iam-auth:2.2.0
</code></pre><p>EMR Serverless automatically downloads these JARs from Maven Central (or another configured repository) at runtime.<br>You do <strong>not</strong> need to manually bundle these JARs unless offline use or strict version requirements apply.</p><hr><h2 id=configuring-spark-streaming-job-for-amazon-msk-iam-authentication>Configuring Spark Streaming job for Amazon MSK IAM authentication</h2><p>In your Spark Streaming application, configure the Kafka source with SASL properties to enable IAM-based authentication.<br>The following code snippet demonstrates the required configuration:</p><pre tabindex=0><code>topic_df = (spark.readStream
    .format(&#34;kafka&#34;)
    .option(&#34;kafka.bootstrap.servers&#34;, kafka_bootstrap_servers)
    .option(&#34;subscribe&#34;, topic_input)
    .option(&#34;startingOffsets&#34;, &#34;earliest&#34;)
    .option(&#34;kafka.security.protocol&#34;,&#34;SASL_SSL&#34;)
    .option(&#34;kafka.sasl.mechanism&#34;,&#34;AWS_MSK_IAM&#34;)
    .option(&#34;kafka.sasl.jaas.config&#34;,&#34;software.amazon.msk.auth.iam.IAMLoginModule required;&#34;)
    .option(&#34;kafka.sasl.client.callback.handler.class&#34;,&#34;software.amazon.msk.auth.iam.IAMClientCallbackHandler&#34;)
    .load()
    .selectExpr(&#34;CAST(value AS STRING)&#34;)
)
</code></pre><p>Key properties:</p><ul><li><strong>kafka.security.protocol = SASL_SSL</strong> – Enables encrypted communication via SSL with SASL authentication.</li><li><strong>kafka.sasl.mechanism = AWS_MSK_IAM</strong> – Instructs Kafka to use IAM-based SASL authentication.</li><li><strong>kafka.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required;</strong> – Specifies AWS-provided login module for IAM integration.</li><li><strong>kafka.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler</strong> – Handles signing and actual IAM authentication.</li></ul><p>With this configuration, Spark uses IAM credentials associated with the EMR Serverless job execution role to authenticate with MSK Serverless — no additional credentials, certificates, or secrets are required.</p><hr><h1 id=processing-data-using-emr-serverless-streaming-job-with-amazon-msk-iam-authentication>Processing data using EMR Serverless streaming job with Amazon MSK IAM authentication</h1><p>Follow these steps to submit a Spark Streaming job that processes data from MSK Serverless:</p><ol><li><p>Submit the Spark Streaming job to EMR Serverless using the AWS Command Line Interface (AWS CLI), which is pre-installed on the EC2 instance.</p></li><li><p>Log in to the EC2 instance via Session Manager.</p><ul><li>Select the instance named <strong>msk-emr-serverless-blog</strong>, then choose <strong>Connect</strong>.</li></ul></li><li><p>Run the following command to submit the streaming job:</p><ul><li>Provide the parameters from the CloudFormation stack output.</li></ul></li></ol><pre tabindex=0><code>sudo su - ec2-user

aws emr-serverless start-job-run \
--application-id &lt;APPLICATION ID&gt; \
--execution-role-arn &lt;EXECUTION ROLE ARN&gt; \
--mode &#39;STREAMING&#39; \
--job-driver &#39;{
&#34;sparkSubmit&#34;: {
&#34;entryPoint&#34;: &#34;s3://&lt;EMR BLOG SCRIPT BUCKET&gt;/emr_pyspark_streaming_script/pysparkStreamingBlog.py&#34;,
&#34;entryPointArguments&#34;:[&#34;--topic_input&#34;,&#34;sales_data_topic&#34;,&#34;--kafka_bootstrap_servers&#34;,&#34;&lt;BOOTSTRAP URL WITH PORT&gt;&#34;,&#34;--output_s3_path&#34;,&#34;s3://&lt;EMR STREAMING OUTPUT BUCKET&gt;/output/sales-order-data/&#34;,&#34;--checkpointLocation&#34;,&#34;s3://&lt;EMR STREAMING OUTPUT BUCKET&gt;/checkpointing/checkpoint-sales-order-data/&#34;,&#34;--database_name&#34;,&#34;emrblog&#34;,&#34;--table_name&#34;,&#34;sales_order_data&#34;],
&#34;sparkSubmitParameters&#34;: &#34;--conf spark.hadoop.hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory --conf spark.executor.cores=2 --conf spark.executor.memory=5g --conf spark.driver.cores=2 --conf spark.driver.memory=5g --conf spark.executor.instances=5 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,software.amazon.msk:aws-msk-iam-auth:2.2.0&#34;
}}&#39;
</code></pre><ol start=4><li><p>After submitting the job, log in to <strong>EMR Studio</strong> using the URL from the <strong>EmrServerlessStudioURL</strong> CloudFormation output.</p></li><li><p>In the navigation pane, choose <strong>Applications</strong> under Serverless.</p></li><li><p>Select the application ID matching the value<br><strong>EmrServerlessSparkApplicationID</strong> from the CloudFormation stack output.</p></li><li><p>In the <strong>Streaming job runs</strong> tab, verify that the job was successfully submitted and wait for it to start running.</p></li></ol><p><img alt="Streaming job runs screenshot" src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/blog-4406-image-7.jpeg></p><hr><h1 id=validating-data-in-athena>Validating data in Athena</h1><p>After the EMR Serverless Spark Streaming job has run and created the processed data table in the Data Catalog, follow these steps to validate the data using Athena:</p><ol><li><p>Open the Athena console and access the query editor.</p></li><li><p>Select Data Catalog as the data source.</p></li><li><p>Select the <strong>emrblog</strong> database — created by the streaming job.</p></li><li><p>To validate the data, run the following query:</p></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span>
</span></span><span style=display:flex><span>    DATE_TRUNC(<span style=color:#e6db74>&#39;minute&#39;</span>, date) <span style=color:#66d9ef>AS</span> minute_window,
</span></span><span style=display:flex><span>    ROUND(<span style=color:#66d9ef>SUM</span>(total_amount), <span style=color:#ae81ff>2</span>) <span style=color:#66d9ef>AS</span> total_amount
</span></span><span style=display:flex><span><span style=color:#66d9ef>FROM</span>
</span></span><span style=display:flex><span>    emrblog.sales_order_data
</span></span><span style=display:flex><span><span style=color:#66d9ef>WHERE</span>
</span></span><span style=display:flex><span>    DATE_TRUNC(<span style=color:#e6db74>&#39;day&#39;</span>, date) <span style=color:#f92672>=</span> <span style=color:#66d9ef>CURRENT_DATE</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>GROUP</span> <span style=color:#66d9ef>BY</span>
</span></span><span style=display:flex><span>    DATE_TRUNC(<span style=color:#e6db74>&#39;minute&#39;</span>, date)
</span></span><span style=display:flex><span><span style=color:#66d9ef>ORDER</span> <span style=color:#66d9ef>BY</span>  
</span></span><span style=display:flex><span>    minute_window <span style=color:#66d9ef>DESC</span>;
</span></span></code></pre></div><p><img alt=athena-query-result src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/blog-4406-image-8.jpg></p><hr><h1 id=clean-up>Clean up</h1><p>To delete the resources and avoid charges, follow these steps:</p><ol><li><p>Log in to EMR Studio using the URL shown in the <strong>EmrServerlessStudioURL</strong> CloudFormation output.</p></li><li><p>In the <strong>navigation pane</strong>, choose <strong>Applications</strong> under Serverless.</p></li><li><p>Select the <strong>application ID</strong> matching the value<br><strong>EmrServerlessSparkApplicationID</strong> in the CloudFormation output.</p></li><li><p>In the <strong>Streaming job runs</strong> tab, select the running job and <strong>cancel</strong> it.</p></li><li><p>Access the <strong>AWS CloudFormation console</strong> and delete the stack named<br><strong>vpc-msk-emr-serverless-studio</strong>.</p></li></ol><hr><h1 id=conclusion>Conclusion</h1><p>In this blog, we presented a serverless pipeline for processing streaming data with IAM authentication, helping you focus on extracting analytics insights instead of managing complex infrastructure.</p><p>You can customize your Spark Streaming code in EMR Serverless to apply transformations and filters, ensuring that clean data is loaded into Amazon S3.</p><p>This solution combines the power of Amazon EMR Serverless Spark Streaming and MSK Serverless, integrated securely through IAM authentication — enabling you to simplify your streaming workflow without managing the integration between Amazon MSK and Amazon EMR Spark Streaming.</p><hr><h1 id=about-the-authors>About the authors</h1><p><img alt=shubham-purwar src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/09/06/shubham-100.jpeg></p><p><strong>Shubham Purwar</strong> is an AWS Analytics Specialist Solution Architect.<br>He helps organizations maximize their data potential by designing and implementing analytics solutions that are scalable, secure, and high-performance on AWS.<br>With deep expertise in AWS analytics services, Shubham works closely with customers to understand unique business requirements and build customized solutions that deliver actionable insights and drive business growth.<br>In his free time, Shubham enjoys spending time with family and traveling around the world.</p><p><img alt=nitin-kumar src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/Nitinnew-100x111.jpg></p><p><strong>Nitin Kumar</strong> is a Cloud Engineer (ETL) at AWS, specializing in AWS Glue.<br>With over 10 years of experience, he excels at helping customers manage large-scale data workloads, focusing on data processing and analytics.<br>He is dedicated to helping customers overcome ETL-related challenges and build scalable data processing and analytics pipelines on AWS.<br>In his free time, Nitin enjoys watching movies, cooking, and spending time with his family.</p><p><img alt=prashanthi-chinthala src=https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/05/26/WhatsApp-Image-2025-05-23-at-9.40.42-PM-100x103.jpeg></p><p><strong>Prashanthi Chinthala</strong> is a Cloud Engineer (DIST) at AWS.<br>She helps customers solve challenges related to Amazon EMR and build scalable data processing and analytics pipelines on AWS.</p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=../../3-blogstranslated/3.2-blog2/ title="Blog 2"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=../../4-eventparticipated/ title="Events Participated" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../../js/clipboard.min.js?1765252493></script><script src=../../js/perfect-scrollbar.min.js?1765252493></script><script src=../../js/perfect-scrollbar.jquery.min.js?1765252493></script><script src=../../js/jquery.sticky.js?1765252493></script><script src=../../js/featherlight.min.js?1765252493></script><script src=../../js/highlight.pack.js?1765252493></script><script>hljs.initHighlightingOnLoad()</script><script src=../../js/modernizr.custom-3.6.0.js?1765252493></script><script src=../../js/learn.js?1765252493></script><script src=../../js/hugo-learn.js?1765252493></script><link href=../../mermaid/mermaid.css?1765252493 rel=stylesheet><script src=../../mermaid/mermaid.js?1765252493></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>